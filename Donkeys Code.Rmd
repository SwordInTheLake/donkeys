---
title: "Donkeys Code"
author: "Eric Tay, Daniel Zhou, Sherry Hu, Jason McEachin"
date: "5/21/2020"
output: html_document
---

```{r} 
library(dplyr) # dplyr==0.8.5
library(ggplot2) # ggplot2==3.2.1
library(olsrr) # olsrr==0.5.3
library(lmtest) # lmtest==0.9-37
library(numbers) # numbers==0.7-5
library(cowplot) # cowplot==1.0.0
library(rms) # rms==5.1-4
library(plyr) # plyr==1.8.5

load(file = "donkeys.Rda")
```

```{r}
# Read in donkeys
donkeys <- data.frame(donkeys)

# Remove outliers and sort based on weight as per Milner and Rouge

# drop BCS = 4.5 and BCS = 1
donkeys <- subset(donkeys, BCS!=4.5 & BCS!=1)

# drop baby donkey 
baby <- subset(donkeys, Weight == 27)
donkeys <- subset(donkeys, Weight != 27)

# Order donkeys by weight
donkeys <- donkeys[order(donkeys$Weight),]
```

```{r}
# We bin all ages > 5, this is easier for the vet to determine, since ages are determined by teeth, or by owner's knowledge
# Doing so also leads to slightly better performance, possibly because Age binning above 5 years was not accurate
levels(donkeys$Age) <- c(levels(donkeys$Age), ">5")
donkeys$Age[donkeys$Age == "5-10"] <- ">5"
donkeys$Age[donkeys$Age == "10-15"] <- ">5"
donkeys$Age[donkeys$Age == "15-20"] <- ">5"
donkeys$Age[donkeys$Age == ">20"] <- ">5"

# We bin BCS = 1.5 and BCS = 2, reasons for doing so will be explained later
donkeys_binned <- donkeys
donkeys_binned$BCS[donkeys_binned$BCS == "1.5"] <- "2"
donkeys_binned$BCS <- revalue(donkeys_binned$BCS, c("2"="1.5/2"))
#levels(donkeys_binned$BCS) <- c(levels(donkeys_binned$BCS),"1.5/2")
#donkeys_binned$BCS[donkeys_binned$BCS == "1.5"] <- "1.5/2"
#donkeys_binned$BCS[donkeys_binned$BCS == "2"] <- "1.5/2"

# train-test split, take every 5th donkey for test set, as per Milner and Rouge
# train/test does not have BCS = 1.5/2 binned together, while train_binned/test_binned does
train <- NULL
test <- NULL 
train_binned <- NULL
test_binned <- NULL 

for(i in 1:nrow(donkeys)){
  if (mod(i,5) == 0){
    test <- rbind(test, donkeys[i,])
    test_binned <- rbind(test_binned, donkeys_binned[i,])
  }
  else{
    train <- rbind(train, donkeys[i,])
    train_binned <- rbind(train_binned, donkeys_binned[i,])
  }
}

# Milner and Rouge trained their data only on donkeys with BCS 3, and Age > 5. 

train_reproducing <- subset(train, BCS==3.0)
train_reproducing <- subset(train_reproducing, Age!='<2')
train_reproducing <- subset(train_reproducing, Age!='2-5')
```

# Reproducing their code

```{r}
# ordinary least squares
model.rep = lm(2*(sqrt(Weight)-1) ~ log(Length) + log(Girth), data=train_reproducing)
summary(model.rep)
```

* Values are different because of the asymmetric loss function they implemented.
* b0_given = -107
* b1_given = 19.91
* b2_given = 7.712
* We will use their given values to reproduce their results, and compare with our proposed model later.

```{r}
f = -107 + 19.91*log(test$Girth)
g = 7.712*log(test$Length)
raw_weight = (((f+g)/2) + 1)^2 #This is predicted weight without additive adjustments
```

```{r}
levels(test$Age)
levels(test$BCS)
```

```{r}
#This implements the additive adjustments
age_adjustment <- function(num){
  list = c(-8,-4,0,0,0,0,0)
  return(list[num])
}
bcs_adjustment <- function(num){
  list = c(NA,-10,-6,-5,0,6,14,NA)
  return(list[num])
}
age = sapply(as.numeric(test$Age),age_adjustment)
bcs = sapply(as.numeric(test$BCS),bcs_adjustment)

author_predicted = raw_weight + age + bcs #This is predicted weight with additive adjustments
```

```{r}
#Plot a histogram of relative errors
plot_hist <- function(predicted, actual){
  actual_over_predicted = actual/predicted
  n = length(actual_over_predicted)
  h <- hist(actual_over_predicted, breaks = 4, main = "Distribution of relative errors, actual/predicted", ylim=c(0,60), xlab = "Relative error, actual/predicted")
  text(h$mids,h$counts,labels=paste(signif(100*h$counts/n,3),"%"), adj=c(0.5, -0.5))
}
```


```{r}
# We reproduce their histogram
plot_hist(author_predicted, test$Weight)
```


```{r}
# We plot the actual versus predicted weights
plot_predictions <- function(predicted){
  ggplot(test, aes(x = predicted, y = Weight)) + geom_point(color = "red") + xlim(75,225) + ylim(75,225) + geom_abline(intercept = 0, slope = 1, size=0.9) + 
  geom_abline(intercept = 0, slope = 1.1, color="gray", linetype = "dashed", size=1) + 
  geom_abline(intercept = 0, slope = .9, color="gray", linetype = "dashed", size=1) + 
  geom_abline(intercept = 0, slope = .8, color="gray", linetype = "dotted", size=1) + 
  geom_abline(intercept = 0, slope = 1.2, color="gray", linetype = "dotted", size=1) + 
  scale_x_continuous(name = "Predicted", breaks = seq(75,225,25), limits = c(75,225)) + 
  scale_y_continuous(name = "Actual Weights", breaks = seq(75,225,25), limits = c(75,225))
}
```

```{r}
# We reproduce their graph 
plot_predictions(author_predicted)
```

* Reproduction of histogram and graph give us confidence that we have selected the correct training and testing samples, and our predictions using their model are accurate. 

# EDA

```{r bcs-age, fig.width=8, fig.height=5}
# Code and idea obtained from Wang, Xu, Zhang

p5 <- ggplot(data = donkeys) +
  geom_bar(mapping = aes(x = BCS, color = BCS, fill = BCS)) + labs(title = "Figure 4 - Distribution pf BCS values")

p6 <- ggplot(data = donkeys) +
  geom_bar(mapping = aes(x = Age, color = Age, fill = Age)) + labs(title = "Figure 5 - Distribution of Age")

plot_grid(p5, p6)
```

```{r}
#Get proportions of BCS and Age Values
prop.table(table(donkeys$BCS)[-c(1,8)])*100
prop.table(table(donkeys$Age)[-c(3,4,5,6)])*100
```

* Less than 1 % of donkeys have BCS 1.5. 
* This might give us motivation to bin BCS = 1.5 with BCS = 2. 
* Proportions for Age do not motivate any binning like the Miler and Rougier.

```{r}
pairs(Weight~Length + Girth + Height, data = train)
```

* Plots support use of linear model
* Let us try some transformations

```{r}
pairs(log(Weight)~log(Length) + log(Girth) + log(Height), data = train)
```

* This motivates the use of log transforms

# Model 2

# Run model

```{r}
model.prop = lm(log(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=train)
summary(model.prop)
vif(model.prop) # Idea obtained from Wang, Xu, Zhang
```

* We note that the beta values for sex is very low, and the p-values are relatively high. This gives us motivation to do variable selection.
* Height seems to be important 
* We also note that BCS = 2 does not seem to be significant, and standard error is almost as large as the estimate. 
* VIF values for BCS = 2.5 and BCS = 3 are above 10, this could indicate multicollinearity.
* Let us bin BCS = 1.5 with BCS = 2

```{r}
#Binned model
model.prop = lm(log(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=train_binned)
summary(model.prop)
vif(model.prop)
```
* Note beta values outside of BCS are not changed significantly
* Values in BCS are changed because of a shift in the baseline category
* VIF values are much better, which provides support for this binning

# Model Selection

```{r}
stepwise = ols_step_both_aic(model.prop)
stepwise

forward = ols_step_forward_aic(model.prop)
summary(forward$model)

backward = ols_step_backward_aic(model.prop)
summary(backward$model)
```

* We see that indeed, sex is irrelevant. However, unlike the original paper, Height seems to be relevant.

# Model 3

# Run model

```{r}
#Remove Sex
model = lm(log(Weight) ~ BCS + Age + log(Length) + log(Girth) + log(Height), data=train_binned)
predictions = predict.lm(model, test_binned, interval="confidence")
our_predicted = exp(predictions[,1])
summary(model)
confint(model)
```

* Height is still important
* We also note that we get $\beta_{10} = 0.610, \beta_{11} = 1.50$, which are not equal to our idealized model, but close. This could indicate a cylinder with a "fatter" center, and a more ellipsoidal body. 

# Diagnostics

```{r}
plot(model)
```

* Everything looks good.

# Fit to Test

```{r}
#Plot Actual weights vs predicted
plot_predictions(our_predicted)
```

```{r}
# Get Mean Squared Error and Mean Relative Squared Error
results <- function(predicted, actual){
  residuals = predicted - actual
  relative_error = predicted/actual
  MSE = mean(residuals^2)
  cat("The MSE is", MSE,'\n')
  cat("The Mean Relative Squared Error is",mean((relative_error-1)^2),'\n\n')
}
```

```{r}
results(our_predicted,test_binned$Weight)
```

# Test for Interactions

```{r}
donkeys_binned_log = subset(donkeys_binned, select = -c(WeightAlt))
donkeys_binned_log$Length = log(donkeys_binned_log$Length)
donkeys_binned_log$Weight = log(donkeys_binned_log$Weight)
donkeys_binned_log$Height = log(donkeys_binned_log$Height)
donkeys_binned_log$Girth = log(donkeys_binned_log$Girth)

names(donkeys_binned_log)[names(donkeys_binned_log) == "Length"] <- "logLength"
names(donkeys_binned_log)[names(donkeys_binned_log) == "Weight"] <- "logWeight"
names(donkeys_binned_log)[names(donkeys_binned_log) == "Height"] <- "logHeight"
names(donkeys_binned_log)[names(donkeys_binned_log) == "Girth"] <- "logGirth"
```

```{r}
train_binned_log <- NULL
test_binned_log <- NULL 

for(i in 1:nrow(donkeys)){
  if (mod(i,5) == 0){
    test_binned_log <- rbind(test_binned_log, donkeys_binned_log[i,])
  }
  else{
    train_binned_log <- rbind(train_binned_log, donkeys_binned_log[i,])
  }
}
```

```{r}
model.2nd = lm(logWeight~ .^2, data=train_binned_log)
model.3rd = lm(logWeight~ .^3, data=train_binned_log)

model_poly_2nd = lm( logWeight ~ Age + Sex + BCS + logLength + logHeight + logGirth + I(logLength ^ 2) + I(logHeight^2) + I(logGirth^2), data=train_binned_log)

model_poly_3rd = lm(logWeight ~ Age + Sex + BCS + logLength + logHeight + logGirth + I(logLength^2) + I(logHeight^2) + I(logGirth^2) + I(logLength^2*logHeight) + I(logLength^2*logGirth) + I(logHeight^2*logLength) + I(logHeight^2*logGirth) + I(logGirth^2*logLength) + I(logGirth^2*logHeight) + I(logLength^3) + I(logHeight^3) + I(logGirth^3), data=train_binned_log)
```

```{r warning=FALSE}
backward_2nd = ols_step_backward_aic(model.2nd)
backward_3rd = ols_step_backward_aic(model.3rd)

backward_poly_2nd = ols_step_backward_aic(model_poly_2nd)
backward_poly_3rd = ols_step_backward_aic(model_poly_3rd)
```

```{r}
names(backward_2nd$model$coefficients) 
names(backward_3rd$model$coefficients) 
names(backward_poly_2nd$model$coefficients) 
names(backward_poly_3rd$model$coefficients) 
```

# Cross-Validation

```{r}
MSEs_model = c()
MRSEs_model = c()
MSEs_backward_2nd = c()
MRSEs_backward_2nd = c()
MSEs_backward_3rd = c()
MRSEs_backward_3rd = c()
MSEs_backward_poly_2nd = c()
MRSEs_backward_poly_2nd = c()
MSEs_backward_poly_3rd = c()
MRSEs_backward_poly_3rd = c()

store_results <- function(model,test_fold){
  predictions = predict.lm(model, test_fold, interval="confidence")
  residuals = exp(predictions[,1]) - exp(test_fold$logWeight)
  MSE = mean(residuals^2)
  relative_error = exp(predictions[,1])/exp(test_fold$logWeight)
  MRSE = (mean((relative_error-1)^2))  
  return(c(MSE,MRSE))
}

for(i in 1:5){
  
  # split into train and test folds
  test_indices = seq(i, nrow(donkeys_binned_log), 5)
  train_fold = donkeys_binned_log[-test_indices,]
  test_fold = donkeys_binned_log[test_indices,]
  
  #create a new model on train_fold
  our_model = lm(logWeight ~ BCS + Age + logLength + logGirth + logHeight, data=train_fold)
  backward_2nd = lm(logWeight ~ Age + logHeight +BCS*logLength + Age*logHeight + logLength*logGirth + logHeight*logGirth, data=train_fold)
  backward_3rd = lm(logWeight ~ BCS + logLength + logHeight + logLength*Age + logLength*logGirth + logLength*Age*logGirth + logLength*logHeight*Age + logHeight*logGirth*Age, data=train_fold)
  backward_poly_2nd = lm(logWeight ~ Age + BCS + logLength + logHeight + logGirth + I(logLength^2) + I(logHeight^2) + I(logGirth^2), data=train_fold)
  backward_poly_3rd = lm(logWeight ~ BCS + Age + logLength + logHeight +logGirth + I(logLength^2) + I(logGirth^2) + I(logLength^2 * logHeight), data=train_fold)
  
  MSEs_model = c(MSEs_model, store_results(our_model,test_fold)[1])
  MRSEs_model = c(MRSEs_model, store_results(our_model,test_fold)[2])
  MSEs_backward_2nd = c(MSEs_backward_2nd, store_results(backward_2nd,test_fold)[1])
  MRSEs_backward_2nd = c(MRSEs_backward_2nd, store_results(backward_2nd,test_fold)[2])
  MSEs_backward_3rd = c(MSEs_backward_3rd, store_results(backward_3rd,test_fold)[1])
  MRSEs_backward_3rd = c(MRSEs_backward_3rd, store_results(backward_3rd,test_fold)[2])
  MSEs_backward_poly_2nd = c(MSEs_backward_poly_2nd, store_results(backward_poly_2nd,test_fold)[1])
  MRSEs_backward_poly_2nd = c(MRSEs_backward_poly_2nd, store_results(backward_poly_2nd,test_fold)[2])
  MSEs_backward_poly_3rd = c(MSEs_backward_poly_3rd, store_results(backward_poly_3rd,test_fold)[1])
  MRSEs_backward_poly_3rd = c(MRSEs_backward_poly_3rd, store_results(backward_poly_3rd,test_fold)[2])
}
```

```{r}
display_result <- function(vec, text){
  cat("The mean", text, "is", mean(vec), "with standard deviation", sd(vec), "\n")
}
```

```{r}
display_result(MSEs_model,"MSE of our model")
display_result(MSEs_backward_2nd,"MSE of pairwise interactions")
display_result(MSEs_backward_3rd,"MSE of three-way interactions")
display_result(MSEs_backward_poly_2nd,"MSE of 2nd order polynomial terms")
display_result(MSEs_backward_poly_3rd,"MSE of 3rd order polynomial terms")
```

```{r}
display_result(MRSEs_model,"MRSE of our model")
display_result(MRSEs_backward_2nd,"MRSE of pairwise interactions")
display_result(MRSEs_backward_3rd,"MRSE of three-way interactions")
display_result(MRSEs_backward_poly_2nd,"MRSE of 2nd order polynomial terms")
display_result(MRSEs_backward_poly_3rd,"MRSE of 3rd order polynomial terms")
```

* There is a reduction in error with adding other interaction/polynomial terms. However, this reduction is not significant, especially when considering the standard deviation of the MSEs and MRSEs. 
* We have opted to continue with our simple model, for its ease of interpretability (it is not immediately interpretable what polynomial log terms represent). However, we believe that adding these terms is a viable approach.
* The good performance of other models indicate that our actions of binning, including Height, and log transform are probably appropriate.

```{r}
plot(1:5, MSEs_model, type="o", main="MSEs")
```

```{r}
plot(1:5, MRSEs_model, type="o", main = "Mean Relative Squared Errors")
```

* We are unable to perform cross-validation on authors' model because we cannot recover loss function
* Cross-validation shows that our model performs even better on other folds. 

# Comparison with Model 1

```{r}
#Compare distributions
plot_hist(author_predicted, test$Weight)
plot_hist(our_predicted, test_binned$Weight)
```

```{r}
#Compare MSE and MRSE
results(author_predicted,test$Weight)
results(our_predicted,test_binned$Weight)
```

* We note a modest reduction in MSE, and a slight reduction in relative error too. Our model is comparable/better than the given model.

* Our model has a slightly higher concentration of residues and relative errors towards the center, as reflected by our calculated values.

# Baby Donkey

```{r}
# We suspect that our model might extend better to out of data samples, especially for baby donkeys.
our_prediction = exp(predict.lm(model, baby)[1])
f = -107 + 19.91*log(baby$Girth)
g = 7.712*log(baby$Length)
raw_weight = (((f+g)/2) + 1)^2
age = sapply(as.numeric(baby$Age),age_adjustment)
bcs = sapply(as.numeric(baby$BCS),bcs_adjustment)
author_prediction = raw_weight + age + bcs
cat("The baby donkey weighs", baby$Weight, 'kg. \n')
cat("Milner and Rougier's model predicts", author_prediction, 'kg. \n')
cat("Our model predicts", our_prediction, 'kg. \n')
```

# What about our performance on their loss function? 

* We are unable to reproduce their loss function exactly, but we can try to approximate it

```{r}
#First transform the variables and store data in X, target variable is y
n = nrow(train_reproducing)
X <- matrix(NA, nrow = n, ncol = 3)
X[,1] <- rep(1,n) #Intercept
X[,2] <- log(train_reproducing$Length)
X[,3] <- log(train_reproducing$Girth)
y <- 2*(sqrt(train_reproducing$Weight) - 1)
theta<-c (-103.394,8.057,18.837) #Initialize base on ols values
```

```{r}
# Define scaled quad-quad loss function that weights negative residuals greater
# We do not implement a tilt as mentioned in the paper, as this would require optimizing over too many hyperparameters

new_cost<-function(X, y, par, alpha){  
  predictions = X%*%par
  relative_error = y/predictions - 1 #Note we use actual/predicted as per Miler and Rougier
  for (i in 1:length(relative_error)){
    if(relative_error[i]<0){
      relative_error[i]=relative_error[i]*alpha #Weight negative relative error
    }
  }
  J <- sum((relative_error)^2)
  return(J) 
}
```

```{r}
# We search for the optimal value that we weight the negative residuals by
alphas = seq(1,2.5,by=0.01)
differences = c()
optimal = c(-107.0, 7.712, 19.91)

for(i in 1:length(alphas)){
  ret = optim(par = theta, fn = new_cost, X = X, y = y, alpha = alphas[i]) #We use an optimizer to solve for beta values given the loss function
  difference = sum(abs(ret$par - optimal)) #We take the difference between the beta values we obtain and the values in the paper
  differences[i] = difference
}

alpha = alphas[which.min(differences)]
print(alpha) #Negative relative errors are weighted by alpha^2
ret = optim(par = theta, fn = new_cost, X = X, y = y, alpha = alpha)
print(ret$par) #Beta values we obtain with this alpha
```

```{r}
#We define the asymmetric loss function
asymmetric_loss <- function(predicted, actual, alpha){
  relative_error = actual/predicted-1
  for (i in 1:length(relative_error)){
    if(relative_error[i]<0){
      relative_error[i]=relative_error[i]*alpha
    }
  }
  cat("The Asymmetric Loss is",mean((relative_error)^2),"\n")
}
```

```{r}
# Generic Assymetric Loss
assym_func <- function(x, alpha){
  if(x < 0){
    return(alpha^2*x^2)
  }
  return(x^2)
}

x <- seq(-0.4,0.4,0.001)
Loss <- lapply(x, assym_func, 2.14)

plot(x,Loss, xlab = "Relative Error, Actual/Predicted", main = "Assymetric Loss Function")
```

```{r}
#Compare asymmetric loss on relative error
asymmetric_loss(author_predicted,test$Weight,alpha)
asymmetric_loss(our_predicted,test_binned$Weight,alpha)
```

* As expected, our model does perform better under the asymmetric loss. 
* We could train our model with this loss, as shown below.

# Train Model with Asymmetric Loss

```{r}
# Transform factor variables into extra columns
BCS2.5 = sapply(donkeys_binned$BCS==2.5, as.numeric)
BCS3.0 = sapply(donkeys_binned$BCS==3.0, as.numeric)
BCS3.5 = sapply(donkeys_binned$BCS==3.5, as.numeric)
BCS4.0 = sapply(donkeys_binned$BCS==4.0, as.numeric)
Age25 = sapply(donkeys_binned$Age == "2-5", as.numeric)
Ageover5 = sapply(donkeys_binned$Age == ">5", as.numeric)
donkeys_binned = cbind(donkeys_binned,BCS2.5, BCS3.0, BCS3.5, BCS4.0, Age25, Ageover5)
```

```{r}
# Reobtain train and test splits
train_assym <- NULL
test_assym <- NULL 
for(i in 1:nrow(donkeys_binned)){
  if (mod(i,5) == 0){
    test_assym <- rbind(test_assym, donkeys_binned[i,])
  }
  else{
    train_assym <- rbind(train_assym, donkeys_binned[i,])
  }
}
```

```{r}
# Transform training data into a numeric matrix
n = nrow(train_assym)
X <- matrix(NA, nrow = n, ncol = 10)
X[,1] <- rep(1,n) #Intercept
X[,2:7] <- as.matrix(train_assym[,9:14])
X[,8] <- log(train_assym$Length)
X[,9] <- log(train_assym$Girth)
X[,10] <- log(train_assym$Height)
y <- train_assym$Weight
theta<-c(model$coefficients) # Initialize theta with our estimates from the ols solution
```

```{r}
# Transform testing data into a numeric matrix
n = nrow(test_assym)
X_test <- matrix(NA, nrow = n, ncol = 10)
X_test[,1] <- rep(1,n) #Intercept
X_test[,2:7] <- as.matrix(test_assym[,9:14])
X_test[,8] <- log(test_assym$Length)
X_test[,9] <- log(test_assym$Girth)
X_test[,10] <- log(test_assym$Height)
```

```{r}
# Define asymmetric cost function
new_cost<-function(X, y, par, alpha){  
  predictions = exp(X%*%par)
  relative_error = y/predictions - 1 #Note we use actual/predicted as per Miler and Rougier
  for (i in 1:length(relative_error)){
    if(relative_error[i]<0){
      relative_error[i]=relative_error[i]*alpha #Weight negative relative error
    }
  }
  J <- sum((relative_error)^2)
  return(J) 
}
```

```{r}
#Obtain new beta values
ret = optim(par = theta, fn = new_cost, X = X, y = y, alpha = alpha)
print(ret$par)
print(model$coefficients)
```

* We note that when the model is trained under asymmetric loss, the beta values do not change significantly.

```{r}
# Obtain results for model trained under asymmetric loss
predictions = exp(X_test%*%(ret$par))
asymmetric_loss(predictions, test_assym$Weight, alpha)
plot_hist(predictions, test_assym$Weight)
results(predictions,test$Weight)
```

* Asymmetric loss on the test data is lower, as expected.
* This indicates that our given model is more flexible. While we do not know the loss function exactly, it is likely that once known, our model can be trained to perform better than the Milner-Rougier model.
* However, the MSE and MRSE (a/p) is greater.
* The histogram shows how relative error (a/p) is now skewed toward postive relative errors (a/p). 
* Without full knowledge of the level of asymmetry, we shall revert to our original model, since the difference in betas is not especially significant,  we can benefit from confidence intervals from the ols solution, and it might be unwise to skew our predictions without more knowledge of the loss function.
* Loss functions for model training can be changed flexibly. We shall demonstrate with L1 loss.

# Training with L1 Loss

```{r}
L1_cost<-function(X, y, par){  
  predictions = X%*%par
  J <- sum(abs(log(y) - predictions))
  return(J) 
}
```

```{r}
ret = optim(par = theta, fn = L1_cost, X = X, y = y)
print(ret$par) # New Beta Values
print(model$coefficients)
predictions = exp(X_test%*%(ret$par))
results(predictions,test_binned$Weight) #Get results for new model
asymmetric_loss(predictions,test_binned$Weight,alpha)
```

* L1 Loss shows greater MSE, MRSE, and Asymmetric Loss than our model. 

# Sensitivity Analysis

## Part 1

* Motivated by the EDA, we fit a linear model and notice that we get the same covariates that are important.

```{r}
model = lm(Weight ~ BCS + Age + Sex + Length + Girth + Height, data=train_binned)

stepwise = ols_step_both_aic(model)
stepwise
```

* We fit something similar to their model and notice the same thing

```{r}
model = lm(sqrt(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=train_binned)

stepwise = ols_step_both_aic(model)
stepwise
```

* We try a naive model that adds a term for the cylinder

```{r}
model = lm(Weight ~ BCS + Age + Sex + Length + Girth + Height + Length*Girth^2, data=train_binned)

stepwise = ols_step_both_aic(model)
stepwise
```

* All models show that we should not consider Sex, but Height seems to be important. 

## Part 2

The log-log model makes our weight predictions very sensitive to the changes in beta. Let us quantify this: 

```{r}
quantile(log(donkeys$Height),probs = c(0.025, 0.5, 0.975))
quantile(log(donkeys$Length),probs = c(0.025, 0.5, 0.975))
quantile(log(donkeys$Girth),probs = c(0.025, 0.5, 0.975))
```

* This indicates that a unit change to a $\beta$ involving Length, Height or Girth, leads to an over $450\%$ change in weight.

# Final Betas

We train the model on the full dataset to provide beta values for future predictions. These values are used in the Shiny App provided.

```{r}
model = lm(log(Weight) ~ BCS + Age + log(Length) + log(Girth) + log(Height), data=donkeys_binned)
summary(model)
```
