---
title: "Donkeys Code"
author: "Eric Tay, Daniel Zhou, Sherry Hu, Jason McEachin"
date: "5/21/2020"
output: html_document
---

```{r} 
library(dplyr) # dplyr==0.8.5
library(ggplot2) # ggplot2==3.2.1
library(olsrr) # olsrr==0.5.3
library(lmtest) # lmtest==0.9-37
library(numbers) # numbers==0.7-5
library(paranomo) # paranomo==1.1
```

```{r}
# Read in donkeys
donkeys <- data.frame(donkeys)

# Remove outliers and sort based on weight as per Milner and Rouge

# drop BCS = 4.5 and BCS = 1
donkeys <- subset(donkeys, BCS!=4.5 & BCS!=1)

# drop baby donkey 
donkeys <- subset(donkeys, Weight != 27)

# Order donkeys by weight
donkeys <- donkeys[order(donkeys$Weight),]

# train-test split, take every 5th donkey for test set, as per Milner and Rouge
train <- NULL
test <- NULL 
for(i in 1:nrow(donkeys)){
  if (mod(i,5) == 0){
    test <- rbind(test, donkeys[i,])
  }
  else{
    train <- rbind(train, donkeys[i,])
  }
}

# Load categorical variables as factors

train$BCS <- as.factor(train$BCS)
train$Age <- as.factor(train$Age)
train$Sex <- as.factor(train$Sex)

test$BCS <- as.factor(test$BCS)
test$Age <- as.factor(test$Age)
test$Sex <- as.factor(test$Sex)

# Milner and Rouge trained their data only on donkeys with BCS 3, and Age > 5. 

train_reproducing <- subset(train, BCS==3.0)
train_reproducing <- subset(train_reproducing, Age!='<2')
train_reproducing <- subset(train_reproducing, Age!='2-5')
```

# Reproducing their code

```{r}
# ordinary least squares
model.rep = lm(2*(sqrt(Weight)-1) ~ log(Length) + log(Girth), data=train_reproducing)
summary(model.rep)
```

* Values are different because of the asymmetric loss function they implemented.
* b0_given = -107
* b1_given = 19.91
* b2_given = 7.712
* We will use their given values to reproduce their results, and compare with our proposed model later.

```{r}
f = -107 + 19.91*log(test$Girth)
g = 7.712*log(test$Length)
raw_weight = (((f+g)/2) + 1)^2 #This is predicted weight without additive adjustments
```

```{r}
levels(test$Age)
levels(test$BCS)
```

```{r}
#This implements the additive adjustments
age_adjustment <- function(num){
  list = c(-8,-4,0,0,0,0)
  return(list[num])
}
bcs_adjustment <- function(num){
  list = c(NA,-10,-6,-5,0,6,14,NA)
  return(list[num])
}
age = sapply(as.numeric(test$Age),age_adjustment)
bcs = sapply(as.numeric(test$BCS),bcs_adjustment)

author_predicted = raw_weight + age + bcs #This is predicted weight with additive adjustments
```

```{r}
#Plot a histogram of relative errors
plot_hist <- function(predicted, actual){
  actual_over_predicted = actual/predicted
  n = length(actual_over_predicted)
  h <- hist(actual_over_predicted, breaks = 4, main = "Distribution of relative errors, actual/predicted", ylim=c(0,55), xlab = "Relative error, actual/predicted")
  text(h$mids,h$counts,labels=paste(signif(100*h$counts/n,3),"%"), adj=c(0.5, -0.5))
}
```


```{r}
# We reproduce their histogram
plot_hist(author_predicted, test$Weight)
```


```{r}
# We plot the actual versus predicted weights
plot_predictions <- function(predicted){
  ggplot(test, aes(x = predicted, y = Weight)) + geom_point(color = "red") + xlim(75,225) + ylim(75,225) + geom_abline(intercept = 0, slope = 1, size=0.9) + 
  geom_abline(intercept = 0, slope = 1.1, color="gray", linetype = "dashed", size=1) + 
  geom_abline(intercept = 0, slope = .9, color="gray", linetype = "dashed", size=1) + 
  geom_abline(intercept = 0, slope = .8, color="gray", linetype = "dotted", size=1) + 
  geom_abline(intercept = 0, slope = 1.2, color="gray", linetype = "dotted", size=1) + 
  scale_x_continuous(name = "Predicted", breaks = seq(75,225,25), limits = c(75,225)) + 
  scale_y_continuous(name = "Actual Weights", breaks = seq(75,225,25), limits = c(75,225))
}
```

```{r}
# We reproduce their graph 
plot_predictions(author_predicted)
```

* Reproduction of histogram and graph give us confidence that we have selected the correct training and testing samples, and our predictions using their model are accurate. 

# EDA

```{r}
pairs(Weight~Length + Girth + Height, data = train)
```

* Plots support use of linear model
* Let us try some transformations

```{r}
pairs(log(Weight)~log(Length) + log(Girth) + log(Height), data = train)
```

* This motivates the use of log transforms

# Model 2

# Run model

```{r}
model.prop = lm(log(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=train)
summary(model.prop)
```

* We note that the beta values for sex is very low, and the p-values are relatively high. This gives us motivation to do variable selection.
* Height seems to be important 

# Backwards Selection

```{r}
model = lm(log(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=train)

stepwise = ols_step_both_aic(model)
stepwise

forward = ols_step_forward_aic(model)
forward

backward = ols_step_backward_aic(model)
backward
```

* We see that indeed, sex is irrelevant. However, unlike the original paper, Height seems to be relevant.

# Model 3

# Run model

```{r}
#Remove sex
model = lm(log(Weight) ~ BCS + Age + log(Length) + log(Girth) + log(Height), data=train)
predictions = predict.lm(model, test, interval="confidence")
our_predicted = exp(predictions[,1])
summary(model)
```

* Height is still important
* We also note that we get $\beta_{10} = 0.611, \beta_{11} = 1.47$, which are not equal to our idealized model, but close. This could indicate a cylinder with a "fatter" center, and a more ellipsoidal body. 

# Diagnostics (ideally we can do this for previous model too)

```{r}
plot(model)
```

* Everything looks good, except scale-location plot. 

```{r}
bptest(model)
```

* Breusch-Pagan test indicates, that at a 5% level of significance, that heteroscedasticity exists, so we should be fine for the scale-location plot.

# Fit to Test

```{r}
#Plot Actual weights vs predicted
plot_predictions(our_predicted)
```

```{r}
# Get Mean Squared Error and Mean Squared Relative Error
results <- function(predicted, actual){
  residuals = predicted - actual
  relative_error = predicted/actual
  MSE = mean(residuals^2)
  cat("The MSE is", MSE,'\n')
  cat("The Mean Squared Relative Error is",mean((relative_error-1)^2),'\n')
}
```

```{r}
results(our_predicted,test$Weight)
```

# Cross-Validation

```{r}
models = c()
MSEs = c()
MSREs = c()

# 5 folds 14 coefficients
betas = matrix(NA, nrow = 5, ncol = 14)

for(i in 1:5){
  
  # split into train and test folds
  test_indices = seq(i, nrow(donkeys), 5)
  train_fold = donkeys[-test_indices,]
  test_fold = donkeys[test_indices,]
  
  # create a new model on train_fold
  model = lm(log(Weight) ~ BCS + Age + log(Length) + log(Girth) + log(Height), data=train_fold)

  # save coefficients
  betas[i,] = model$coefficients
  models = c(models, model)
  
  # save MSE and mean relative error
  predictions = predict.lm(model, test_fold, interval="confidence")
  residuals = exp(predictions[,1]) - test_fold$Weight
  MSE = mean(residuals^2)
  relative_error = exp(predictions[,1])/test_fold$Weight
  MSRE = (mean((relative_error-1)^2))
  
  MSEs = c(MSEs, MSE)
  MSREs = c(MSREs, MSRE)
  
}

colnames(betas) = names(model$coefficients)

```

```{r}
mean(MSEs)
sd(MSEs)
```

```{r}
mean(MSREs)
sd(MSREs)
```

```{r}
plot(1:5, MSEs, type="o", main="MSEs")
```

```{r}
plot(1:5, MSREs, type="o", main = "Mean Squared Relative Errors")
```

* We are unable to perform cross-validation on authors' model because we cannot recover loss function
* Cross-validation shows that our model performs even better on other folds. 

# Comparison with Model 1

```{r}
#Compare distributions
plot_hist(author_predicted, test$Weight)
plot_hist(our_predicted, test$Weight)
```


```{r}
#Compare MSE and MSRE
results(author_predicted,test$Weight)
results(our_predicted,test$Weight)
```

* We note a slight reduction in MSE, and in relative error too. Our model is comparable/better than the given model.

* Our model has a slightly higher concentration of residues and relative errors towards the center, as reflected by our calculated values. It is significant to notice that this difference is more pronounced for negative residues/relative errors. If we assume an assymmetric loss function (as shown in the graph) that weights negative residues/errors more heavily, our model would be even better, despite us training on a symmetric loss function. 

# What about our performance on their loss function? 

* We are unable to reproduce their loss function exactly, but we can try to approximate it

```{r}
#First transform the variables and store data in X, target variable is y
n = nrow(train_reproducing)
X <- matrix(NA, nrow = n, ncol = 3)
X[,1] <- rep(1,n) #Intercept
X[,2] <- log(train_reproducing$Length)
X[,3] <- log(train_reproducing$Girth)
y <- 2*(sqrt(train_reproducing$Weight) - 1)
theta<-c (-103.394,8.057,18.837) #Initialize base on ols values
```

```{r}
# Define scaled quad-quad loss function that weights negative residuals greater
# We do not implement a tilt as mentioned in the paper, as this would require optimizing over too many hyperparameters

new_cost<-function(X, y, par, alpha){  
  m <- length(y)
  predictions = X%*%par
  relative_error = y/predictions - 1 #Note we use actual/predicted as per Miler and Rougier
  for (i in 1:length(relative_error)){
    if(relative_error[i]<0){
      relative_error[i]=relative_error[i]*alpha #Weight negative relative error
    }
  }
  J <- sum((relative_error)^2)
  return(J) 
}
```


```{r}
# We search for the optimal value that we weight the negative residuals by
alphas = seq(1,2.5,by=0.01)
differences = c()
optimal = c(-107.0, 7.712, 19.91)

for(i in 1:length(alphas)){
  ret = optim(par = theta, fn = new_cost, X = X, y = y, alpha = alphas[i]) #We use an optimizer to solve for beta values given the loss function
  difference = sum(abs(ret$par - optimal)) #We take the difference between the beta values we obtain and the values in the paper
  differences[i] = difference
}

alpha = alphas[which.min(differences)]
print(alpha) #Negative relative errors are weighted by alpha^2
ret = optim(par = theta, fn = new_cost, X = X, y = y, alpha = alpha)
print(ret$par) #Beta values we obtain with this alpha
```

```{r}
#We define the asymmetric loss function
asymmetric_loss <- function(predicted, actual, alpha){
  relative_error = predicted/actual-1
  for (i in 1:length(relative_error)){
    if(relative_error[i]<0){
      relative_error[i]=relative_error[i]*alpha
    }
  }
  cat("The MSRE is",mean((relative_error)^2),"\n")
}
```

```{r}
#Compare asymmetric loss on relative error
asymmetric_loss(author_predicted,test$Weight,alpha)
asymmetric_loss(our_predicted,test$Weight,alpha)
```

* As expected, our model does perform markedly better under the asymmetric loss. 
* We could train our model with this loss, but this was not done because we felt it was excessive to do so without knowledge of the true loss function. 
* We hence proceed with a model that minimizes absolute squared error. This can be altered as necessary with more information about potential dangers associated with over/under-predictions, and whether relative error is more appropriate. 


# Sensitivity Analysis

## Part 1

* Motivated by the EDA, we fit a linear model and notice that we get the same covariates that are important.

```{r}
model = lm(Weight ~ BCS + Age + Sex + Length + Girth + Height, data=train)

stepwise = ols_step_both_aic(model)
stepwise
```

* We fit something similar to their model and notice the same thing

```{r}
model = lm(sqrt(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=train)

stepwise = ols_step_both_aic(model)
stepwise
```

* We try a naive model that adds a term for the cylinder

```{r}
model = lm(Weight ~ BCS + Age + Sex + Length + Girth + Height + Length*Girth^2, data=train)

stepwise = ols_step_both_aic(model)
stepwise
```

* All models show that we should not consider Sex, but Height seems to be important. 

## Part 2

The log-log model makes our weight predictions very sensitive to the changes in beta. Let us quantify this: 

```{r}
quantile(log(donkeys$Height),probs = c(0.025, 0.5, 0.975))
quantile(log(donkeys$Length),probs = c(0.025, 0.5, 0.975))
quantile(log(donkeys$Girth),probs = c(0.025, 0.5, 0.975))
```

* This indicates that a unit change to a $\beta$ involving Length, Height or Girth, leads to an over $450\%$ change in weight.

# Final Betas

We train the model on the full dataset to provide beta values for future predictions. These values are used in the Shiny App provided.

```{r}
model.prop = lm(log(Weight) ~ BCS + Age + Sex + log(Length) + log(Girth) + log(Height), data=donkeys)
summary(model.prop)
```





